{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":175,"outputs":[{"output_type":"stream","text":"/kaggle/input/train-1/train-2.npy\n/kaggle/input/train-1/train-4.npy\n/kaggle/input/train-1/train-3.npy\n/kaggle/input/train-1/train-1.npy\n/kaggle/input/datasets-chinese-recognition/random_labels.csv\n/kaggle/input/datasets-chinese-recognition/train-2.npy/train-2.npy\n/kaggle/input/datasets-chinese-recognition/test.npy/test.npy\n/kaggle/input/datasets-chinese-recognition/train-4.npy/train-4.npy\n/kaggle/input/datasets-chinese-recognition/train-3.npy/train-3.npy\n/kaggle/input/datasets-chinese-recognition/train-1.npy/train-1.npy\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nfrom keras import backend as K\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nprint(tf.__version__)\nprint(keras.__version__)","execution_count":176,"outputs":[{"output_type":"stream","text":"2.1.0-rc0\n2.3.1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reset_tf_session():\n    curr_session = tf.compat.v1.get_default_session()\n    if curr_session is not None:\n        curr_session.close()\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto\n    config.gpu_options.allow_growth = True\n    s = tf.compat.v1.InteractiveSession(config=config)\n    tf.compat.v1.keras.backend.set_session(s)\n    return s","execution_count":180,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = np.load(\"/kaggle/input/train-1/train-1.npy\", allow_pickle=True)\nfor i in range(2, 5):\n    t = np.load(f\"/kaggle/input/train-1/train-{i}.npy\", allow_pickle=True)\n    data_train = np.concatenate([data_train, t])\ndata_test = np.load('/kaggle/input/datasets-chinese-recognition/test.npy/test.npy', allow_pickle=True)","execution_count":181,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = np.unique([v[1] for v in data_train])\nto_char = {id:char for id, char in enumerate(t)}\nto_id = {char:id for id, char in enumerate(t)}\n","execution_count":182,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_size = 0.3\n\ndef train_gen():\n    for img, label in data_train[int((val_size * len(data_train))):]:\n        img = img[:, :, np.newaxis] # [batch, w, h, channels]\n        label = to_id[label]\n        yield img, label\n        \ndef preprocess_train(img, label):\n    img = tf.image.resize(img, (80, 80))\n    img = tf.cast(img, tf.float32)\n    img = img / 255 - 0.5\n    label = tf.one_hot(label, 1000)\n    return img, label\n\n\ndef val_gen():\n    for img, label in data_train[:int(val_size * len(data_train))]:\n        img = img[:, :, np.newaxis]\n        label = to_id[label]\n        yield img, label\n        \ndef preprocess_val(img, label):\n    img = tf.image.resize(img, (80, 80))\n    img = tf.cast(img, tf.float32)\n    img = img / 255 - 0.5\n    label = tf.one_hot(label, 1000)\n    return img, label\n\ndef test_gen():\n    for img in data_test:\n        img = img[:, :, np.newaxis] # [batch, w, h, channels]\n        yield img\n\ndef preprocess_test(img):\n    img = tf.image.resize(img, (80, 80))\n    img = tf.cast(img, tf.float32)\n    img = img / 255 - 0.5\n    return img","execution_count":183,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_train = tf.data.Dataset.from_generator(train_gen,\n                                          output_types=(tf.uint8, tf.int32),\n                                          output_shapes=((None,None,1), ()))\nds_train = ds_train\\\n.map(preprocess_train, num_parallel_calls=-1)\\\n.shuffle(1024)\\\n.repeat()\\\n.prefetch(-1)\\\n.batch(256)\n\nds_val = tf.data.Dataset.from_generator(val_gen,\n                                          output_types=(tf.uint8, tf.int32),\n                                          output_shapes=((None,None,1), ()))\n\nds_val = ds_val\\\n.map(preprocess_val, num_parallel_calls=-1)\\\n.shuffle(1024)\\\n.repeat()\\\n.prefetch(-1)\\\n.batch(256)\n","execution_count":184,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_test = tf.data.Dataset.from_generator(test_gen,\n                                          output_types=tf.uint8,\n                                          output_shapes=((None,None,1)))\n\nds_test = ds_test\\\n.map(preprocess_test, num_parallel_calls=-1)\\\n.prefetch(-1)\\\n.batch(256)","execution_count":203,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model(): \n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(filters=64, padding='same', kernel_size=(3,3), input_shape=(80, 80, 1), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.Conv2D(filters=64, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))  \n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(filters=64, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.Conv2D(filters=64, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))  \n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(filters=64, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.Conv2D(filters=64, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))  \n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same')) \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.Conv2D(filters=128, padding='same', kernel_size=(3,3), kernel_initializer='he_uniform'))  \n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same')) \n    model.add(tf.keras.layers.BatchNormalization()) \n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(512, kernel_initializer='he_uniform'))\n    model.add(tf.keras.layers.LeakyReLU(0.1))\n    model.add(tf.keras.layers.BatchNormalization())    \n    model.add(tf.keras.layers.Dense(1000, kernel_initializer='he_uniform'))             \n    model.add(tf.keras.layers.Activation(\"softmax\"))\n    \n    return model","execution_count":186,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_model(INIT_LR=5e-3, BATCH_SIZE=256, epoch=20):\n \n    def lr_scheduler(epoch):\n        return INIT_LR * 0.9 ** epoch  \n\n    model = make_model()\n    \n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=tf.keras.optimizers.Adamax(lr=INIT_LR),\n        metrics=['accuracy']\n    )\n  \n    class LrHistory(tf.keras.callbacks.Callback):\n\n        def on_epoch_begin(self, epoch, logs={}):\n            print(\"Learning_rate:\", tf.keras.backend.get_value(model.optimizer.lr))\n\n            \n    \n    history = model.fit(ds_train,\n        epochs=epoch,\n        callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler),\n                   LrHistory(),],\n        validation_data=ds_val,                   \n        shuffle=True,\n        verbose=1,\n        initial_epoch=0, validation_steps=int(val_size*data_train.shape[0]/(BATCH_SIZE)), steps_per_epoch= int((1-val_size)*data_train.shape[0]/(BATCH_SIZE)))\n\n    return history","execution_count":187,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_model()","execution_count":188,"outputs":[{"output_type":"stream","text":"Train for 910 steps, validate for 390 steps\nLearning_rate: 0.005\nEpoch 1/20\n910/910 [==============================] - 259s 285ms/step - loss: 1.1457 - accuracy: 0.7657 - val_loss: 0.3734 - val_accuracy: 0.8963\nLearning_rate: 0.0045\nEpoch 2/20\n910/910 [==============================] - 194s 213ms/step - loss: 0.1674 - accuracy: 0.9537 - val_loss: 0.2621 - val_accuracy: 0.9252\nLearning_rate: 0.00405\nEpoch 3/20\n910/910 [==============================] - 192s 211ms/step - loss: 0.0995 - accuracy: 0.9718 - val_loss: 0.1697 - val_accuracy: 0.9530\nLearning_rate: 0.003645\nEpoch 4/20\n910/910 [==============================] - 194s 213ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.1312 - val_accuracy: 0.9634\nLearning_rate: 0.0032805\nEpoch 5/20\n910/910 [==============================] - 192s 211ms/step - loss: 0.0357 - accuracy: 0.9900 - val_loss: 0.0843 - val_accuracy: 0.9784\nLearning_rate: 0.00295245\nEpoch 6/20\n910/910 [==============================] - 193s 212ms/step - loss: 0.0203 - accuracy: 0.9946 - val_loss: 0.0829 - val_accuracy: 0.9788\nLearning_rate: 0.002657205\nEpoch 7/20\n910/910 [==============================] - 195s 215ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 0.0682 - val_accuracy: 0.9838\nLearning_rate: 0.0023914846\nEpoch 8/20\n910/910 [==============================] - 195s 214ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0498 - val_accuracy: 0.9896\nLearning_rate: 0.002152336\nEpoch 9/20\n910/910 [==============================] - 192s 211ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0469 - val_accuracy: 0.9902\nLearning_rate: 0.0019371024\nEpoch 10/20\n910/910 [==============================] - 193s 212ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0445 - val_accuracy: 0.9909\nLearning_rate: 0.0017433922\nEpoch 11/20\n910/910 [==============================] - 193s 212ms/step - loss: 8.8072e-04 - accuracy: 0.9999 - val_loss: 0.0427 - val_accuracy: 0.9919\nLearning_rate: 0.0015690529\nEpoch 12/20\n910/910 [==============================] - 194s 213ms/step - loss: 6.1089e-04 - accuracy: 0.9999 - val_loss: 0.0417 - val_accuracy: 0.9922\nLearning_rate: 0.0014121477\nEpoch 13/20\n910/910 [==============================] - 194s 213ms/step - loss: 3.8564e-04 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9923\nLearning_rate: 0.001270933\nEpoch 14/20\n910/910 [==============================] - 192s 211ms/step - loss: 3.4568e-04 - accuracy: 1.0000 - val_loss: 0.0415 - val_accuracy: 0.9922\nLearning_rate: 0.0011438397\nEpoch 15/20\n910/910 [==============================] - 193s 212ms/step - loss: 2.1947e-04 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9920\nLearning_rate: 0.0010294557\nEpoch 16/20\n910/910 [==============================] - 194s 213ms/step - loss: 1.6511e-04 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 0.9922\nLearning_rate: 0.0009265101\nEpoch 17/20\n910/910 [==============================] - 196s 216ms/step - loss: 1.8040e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9923\nLearning_rate: 0.0008338591\nEpoch 18/20\n910/910 [==============================] - 193s 212ms/step - loss: 1.3491e-04 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9924\nLearning_rate: 0.0007504732\nEpoch 19/20\n910/910 [==============================] - 193s 212ms/step - loss: 1.1769e-04 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9924\nLearning_rate: 0.00067542586\nEpoch 20/20\n910/910 [==============================] - 193s 213ms/step - loss: 1.2857e-04 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9925\n","name":"stdout"},{"output_type":"execute_result","execution_count":188,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f974f6f1d68>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = make_model()\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=tf.keras.optimizers.Adamax(lr=5e-3),\n        metrics=['accuracy']\n    )\nmodel.fit(ds_train,\n        epochs=20,\n        validation_data=ds_val,                   \n        shuffle=True,\n        verbose=1,\n        initial_epoch=0, validation_steps=int(val_size*data_train.shape[0]/(256)), steps_per_epoch= int((1-val_size)*data_train.shape[0]/(256)))\n#sub = model.predict(ds_test) - это то, из за чего ошибка выдалась, перепрогружать не буду, очень долго\n#sub ","execution_count":201,"outputs":[{"output_type":"stream","text":"Train for 910 steps, validate for 390 steps\nEpoch 1/20\n910/910 [==============================] - 191s 210ms/step - loss: 1.1299 - accuracy: 0.7688 - val_loss: 2.9846 - val_accuracy: 0.4273\nEpoch 2/20\n910/910 [==============================] - 188s 206ms/step - loss: 0.1824 - accuracy: 0.9489 - val_loss: 0.2665 - val_accuracy: 0.9264\nEpoch 3/20\n910/910 [==============================] - 190s 208ms/step - loss: 0.1196 - accuracy: 0.9655 - val_loss: 0.2402 - val_accuracy: 0.9326\nEpoch 4/20\n910/910 [==============================] - 187s 206ms/step - loss: 0.0832 - accuracy: 0.9754 - val_loss: 0.1786 - val_accuracy: 0.9499\nEpoch 5/20\n910/910 [==============================] - 186s 204ms/step - loss: 0.0593 - accuracy: 0.9824 - val_loss: 0.1230 - val_accuracy: 0.9670\nEpoch 6/20\n910/910 [==============================] - 187s 206ms/step - loss: 0.0425 - accuracy: 0.9871 - val_loss: 0.1308 - val_accuracy: 0.9655\nEpoch 7/20\n910/910 [==============================] - 186s 205ms/step - loss: 0.0325 - accuracy: 0.9902 - val_loss: 0.1206 - val_accuracy: 0.9686\nEpoch 8/20\n910/910 [==============================] - 187s 205ms/step - loss: 0.0244 - accuracy: 0.9926 - val_loss: 0.0863 - val_accuracy: 0.9782\nEpoch 9/20\n910/910 [==============================] - 187s 206ms/step - loss: 0.0190 - accuracy: 0.9942 - val_loss: 0.0889 - val_accuracy: 0.9781\nEpoch 10/20\n910/910 [==============================] - 187s 205ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.0776 - val_accuracy: 0.9820\nEpoch 11/20\n910/910 [==============================] - 186s 204ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 0.0695 - val_accuracy: 0.9845\nEpoch 12/20\n910/910 [==============================] - 185s 203ms/step - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.0711 - val_accuracy: 0.9843\nEpoch 13/20\n910/910 [==============================] - 188s 207ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0725 - val_accuracy: 0.9846\nEpoch 14/20\n910/910 [==============================] - 186s 204ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0710 - val_accuracy: 0.9856\nEpoch 15/20\n910/910 [==============================] - 186s 204ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0697 - val_accuracy: 0.9855\nEpoch 16/20\n910/910 [==============================] - 188s 207ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0797 - val_accuracy: 0.9833\nEpoch 17/20\n910/910 [==============================] - 187s 205ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0657 - val_accuracy: 0.9868\nEpoch 18/20\n910/910 [==============================] - 189s 208ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0660 - val_accuracy: 0.9875\nEpoch 19/20\n910/910 [==============================] - 191s 210ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0671 - val_accuracy: 0.9874\nEpoch 20/20\n910/910 [==============================] - 190s 209ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.0675 - val_accuracy: 0.9873\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"in converted code:\n\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py:671 map_fn\n        batch_size=None)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:2371 _standardize_tensors\n        exception_prefix='input')\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py:573 standardize_input_data\n        'with shape ' + str(data_shape))\n\n    ValueError: Error when checking input: expected conv2d_193_input to have 4 dimensions, but got array with shape (None, None, 80, 80, 1)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-201-a7d2477bc40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         initial_epoch=0, validation_steps=int(val_size*data_train.shape[0]/(256)), steps_per_epoch= int((1-val_size)*data_train.shape[0]/(256)))\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    698\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, standardize_function, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;31m# Note that the dataset instance is immutable, its fine to reusing the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mstandardize_function\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    676\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1592\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3926\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3927\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3928\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3929\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3146\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3148\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3139\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3140\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3141\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3081\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py:671 map_fn\n        batch_size=None)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py:2371 _standardize_tensors\n        exception_prefix='input')\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py:573 standardize_input_data\n        'with shape ' + str(data_shape))\n\n    ValueError: Error when checking input: expected conv2d_193_input to have 4 dimensions, but got array with shape (None, None, 80, 80, 1)\n"]}]},{"metadata":{},"cell_type":"markdown","source":"Да, тут ошибка выдалась, но к счастью сама модель обучилась, а проблему я устранила и ниже всё норм. Оно грузит больше часа, пожалейте...... не стала перезагружать"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = model.predict(ds_test)","execution_count":231,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = sub.argmax(1)","execution_count":232,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":228,"outputs":[{"output_type":"execute_result","execution_count":228,"data":{"text/plain":"array([802, 846, 905, ..., 948, 263,  22])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = [to_char[i] for i in sub]\ncsv = pd.read_csv('/kaggle/input/datasets-chinese-recognition/random_labels.csv')\ncsv['Category'] = sub\ncsv.to_csv(\"submission.csv\", index=False)","execution_count":229,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncsv.head()","execution_count":230,"outputs":[{"output_type":"execute_result","execution_count":230,"data":{"text/plain":"   Id  Category\n0   1     63955\n1   2     64432\n2   3     64709\n3   4     64177\n4   5     61881","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>63955</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>64432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>64709</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>64177</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>61881</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Это было жостка. Если я что-то случайно удалила что нужно было или оставила какую-то ненужную штуку, то простите, нет сил и времени в порядок тетрадь привести........... на кегеле сабмитнула, всё ок"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}